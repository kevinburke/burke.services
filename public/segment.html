<!doctype html>
<head>
  <title>Kevin Burke - Consulting</title>
  <link rel="stylesheet" type="text/css" href="/markdown.css">
  <meta name="viewport" content="width=device-width, initial-scale=1">
</head>
<body>
<h1>Case study: Segment</h1>
<p>Segment is an analytics company. You put Segment's tracking code on your
website and they'll send the data to any other analytics tool you want (Google
Analytics, Mixpanel, Salesforce, etc.)</p>
<p>This entails proxying a lot of customer traffic and Segment has <a href="https://segment.com/blog/the-million-dollar-eng-problem/">a
correspondingly large AWS bill</a>. Segment had two problems relating to this
large bill.</p>
<p>One, it was difficult to notice if one part of the engineering organization
suddenly started spending a lot more on AWS. Segment's AWS bill is six figures
per month. The charges for each AWS component making up the bill change as
customers churn, cost inefficiencies are found and fixed, and the engineering
organization deploys new tools. In this environment it can be difficult to
notice that, say, a single team spent $20,000 more on S3 this month than they
did last month.</p>
<p>Two, it's difficult for Segment to predict how much new customers will cost.
Customers are good at predicting how much traffic they will have, the Segment
products they want to use, and the mix of traffic they will send to different
services. Segment had trouble translating this usage information to a dollar
figure. Ideally they wanted to be able to say &quot;1 million new API calls will
cost us $X so we should make sure we are charging at least $Y.&quot;</p>
<p>Segment hired <a href="https://burke.services">me</a> to help them make sense of their AWS bill and help
them determine how much new customers would cost, so they could get out ahead
of their bill and avoid accidental undercharging instead of reacting to
increases in costs. The main goal was to figure out how much of the AWS bill was
attributable to Segment's different product areas, widely defined as:</p>
<ul>
<li>integrations (the code that writes from Segment to various analytics providers)</li>
<li>API (customers browsers sending data to Segment)</li>
<li>warehouses (writing data from Segment <a href="https://segment.com/warehouses">to a customer's data warehouse</a>)</li>
<li>the website and CDN</li>
<li>internal (Support logic for the four above)</li>
</ul>
<h3>The AWS Billing CSV</h3>
<p>There's a setting in the billing portal you can enable where Amazon will write
a CSV with detailed billing information to S3 every day. By detailed I mean
VERY detailed. Here is a typical billing row.</p>
<pre><code>record_type       | LineItem
record_id         | 60280491644996957290021401
product_name      | Amazon DynamoDB
rate_id           | 0123456
subscription_id   | 0123456
pricing_plan_id   | 0123456
usage_type        | USW2-TimedStorage-ByteHrs
operation         | StandardStorage
availability_zone | us-west-2
reserved_instance | N
item_description  | $0.25 per GB-Month of storage used beyond first 25 free GB-Months
usage_start_date  | 2017-02-07 03:00:00
usage_end_date    | 2017-02-07 04:00:00
usage_quantity    | 6e-08
blended_rate      | 0.24952229400
blended_cost      | 0.00000001000
unblended_rate    | 0.25000000000
unblended_cost    | 0.00000001000
resource_id       | arn:aws:dynamodb:us-west-2:012345:table/a-table
statement_month   | 2017-02-01
</code></pre>
<p>That's a charge for a whopping $0.00000001, or one one-millionth of a penny, for
DynamoDB storage on a single table between 3AM and 4AM on February 7th. There
are about six million rows in Segment's billing CSV for a typical month.</p>
<p>Segment was already using <a href="https://github.com/heroku/awsdetailedbilling">Heroku's <code>awsdetailedbilling</code> tool</a>
to copy the billing data from S3 to Redshift. This was a good first step but
lacking in one crucial way. Different parts of Segment's infrastructure used
the same AWS products, and Segment didn't have a great way to break out an AWS
product's costs into its own product area groups. For example, many different
teams use DynamoDB, Elasticache and S3, so it's hard to look at an increased
DynamoDB bill and ascribe it to a given team.</p>
<p>Crucially, about 60% of the bill is for EC2. Segment's engineering team makes
heavy use of <a href="https://aws.amazon.com/ecs/">ECS (Elastic Container Service)</a> instances, running on hosts
in several different pools. A typical pool may have 20 EC2 c1.xlarge instances,
running 200 containers.</p>
<pre><code>(insert fancy diagram of ECS pools here)
</code></pre>
<p>Amazon bills <em>only</em> for the EC2 instance costs, so Segment had zero visibility
into the costs of its container services - how many containers they were running
at a typical time, how much of the pool they were using, and how many CPU and
memory units they were using.</p>
<h3>Cost Allocation Tags</h3>
<p>The most obvious thing to start doing was to use AWS's <a href="http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html">cost allocation
tags</a>. These let you apply a tag to a resource, like an
S3 bucket or a DynamoDB table. Toggle a setting in the AWS billing console and
after a day or so, your chosen tag (we chose <code>product_area</code>) will start showing
up as a new column next to the associated resources in the billing CSV.</p>
<p>There were two challenges: 1) tagging all of the existing infrastructure, and 2)
ensuring that any new resources would automatically have tags. Tagging all of
the infrastructure was pretty easy: For a given AWS product, ask Redshift for
the highest cost resources, then bug people in Slack until they tell you how
they should be tagged, and stop when you've tagged 90% or more of the resources
by cost.</p>
<p>Ensuring that new resources would be tagged as they were added was a little
trickier. Segment uses <a href="https://www.terraform.io/">Terraform</a> to manage AWS resources. In most
cases, Terraform's configuration supports adding the same cost allocation tags
that you can add via the AWS console. Here's an example Terraform configuration
for a S3 bucket:</p>
<pre><code class="language-hcl">resource &quot;aws_s3_bucket&quot; &quot;staging_table&quot; {
  bucket = &quot;segment-tasks-to-redshift-staging-tables-prod&quot;

  tags {
    product_area = &quot;data-analysis&quot;
  }
}
</code></pre>
<p>So I wanted to verify that every time someone wrote <code>resource &quot;aws_s3_bucket&quot;</code>
into a Terraform file, they included a <code>product_area</code> tag. Fortunately Terraform
configurations are written in <a href="https://github.com/hashicorp/hcl">HCL</a>, which ships with <a href="https://kev.inburke.com/kevin/more-comment-preserving-configuration-parsers/">a comment preserving
configuration parser</a>. So I wrote a checker that walks
every Terraform file looking for taggable resources lacking a <code>product_area</code>
tag.</p>
<pre><code class="language-go">func checkItemHasTag(item *ast.ObjectItem, resources map[string]bool) error {
	// looking for &quot;resource&quot; &quot;aws_s3_bucket&quot; or similar
	if len(item.Keys) &lt; 2 { return nil }
	resource, ok := hclchecker.StringKey(item.Keys[1].Token)
	if !ok { return nil }
	if resource != &quot;aws_s3_bucket&quot; { return nil }
	t, ok := item.Val.(*ast.ObjectType)
	if !ok {
		return fmt.Errorf(&quot;bad type: %#v&quot;, item.Val)
	}
	tags, ok := hclchecker.GetNodeForKey(t.List, &quot;tags&quot;)
	if !ok {
		return fmt.Errorf(&quot;aws_s3_bucket resource has no tags&quot;, resource)
	}
	t2, ok := tags.(*ast.ObjectType)
	if !ok {
		return fmt.Errorf(&quot;expected 'tags' to be an ObjectType, got %#v&quot;, tags)
	}
	productNode, ok := hclchecker.GetNodeForKey(t2.List, &quot;product_area&quot;)
	if !ok {
		return errors.New(&quot;Could not find a 'product_area' tag for S3 resource. Be sure to tag your resource with a product_area&quot;)
	}
}
</code></pre>
<p>I set up continuous integration for the repo with Terraform configs, and then
added these checks, so the tests will fail if anyone tries to check in a
taggable resource that's not tagged with a product area. This isn't perfect
since people can still create resources in the AWS console, but it's good enough
for now.</p>
<h4>Rolling up cost allocation tag data</h4>
<p>Once you've tagged resources, accounting for them is pretty simple.</p>
<ol>
<li>Find the <code>product_area</code> tags for each resource, so you have a map of resource
id =&gt; product area tags.</li>
<li>Sum the unblended costs for each resource.</li>
<li>Sum those costs by product area, and write the result to a rollup table.</li>
</ol>
<p>We were able to account for about 35% of the bill using traditional cost
allocation tags.</p>
<h3>No Cost Allocation Tags</h3>
<p>Other AWS resources, notably ECS, <em>don't</em> support cost allocation tags. These
involved a much more Rube Goldberg-ian workflow to get the data into Redshift.
The core of it is:</p>
<ol>
<li>
<p>Set up a <a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html">Cloudwatch subscription</a> any time an ECS task gets
started or stopped.</p>
</li>
<li>
<p>Push the relevant data (Service name, CPU/memory usage, starting or stopping,
EC2 instance ID) from the event to Kinesis Firehose (to aggregate individual
events).</p>
</li>
<li>
<p>Push the data from Kinesis Firehose to Redshift.</p>
</li>
</ol>
<p>Then we multiply the amount of time a given ECS task ran (say, 120 seconds)
by the number of CPU units it used on that machine (up to 4096 - this info is
available in the <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html">task definition</a>), to get a number of CPU-seconds
for each service that ran on the instance. The total bill for the instance is
then divided across services according to the number of CPU-seconds each one
used.</p>
<p>That's not a perfect method. EC2 instances aren't running at 100% capacity all
the time, and the excess currently gets divided across the services running on
the instance, which may or may not be the right culprits for that overhead. But
(and you may recognize this as a common theme in this post), it's good enough.</p>
<h4>Product areas per service</h4>
<p>We want to figure out the right product area for each ECS service, but we can't
tag those services in AWS because ECS doesn't support cost allocation tags.</p>
<p>Instead we added a <code>product_area</code> key to the Terraform module for each ECS
service. These don't lead to any metadata being sent to AWS, since we can't. But
I wrote a script that reads the <code>product_area</code> keys for each service, and then
publishes the service name =&gt; product area mappings to DynamoDB on every new
push to the master branch. Tests validate that each service has been tagged
with a product area.</p>
<p>It's not perfect, and it's not great that it's not hooked in to the existing
Terraform or AWS infrastructure in any way - the parser has required a lot of
massaging to avoid false positives as people add new module definitions. But
it's good enough for now.</p>
<h4>EBS</h4>
<p>Elastic Block Storage (EBS) makes up a significant part of the bill. EBS volumes
are typically attached to an EC2 instance, and for accounting purposes it makes
sense to consider the instance and the volume as a single cost. But the AWS
billing CSV doesn't show you which EBS volume was attached to which instance,
and we can't tag the majority of EBS volumes with a single product area, since
they're supporting the many different ECS services running on an EC2 instance.</p>
<p>We again used Cloudwatch for this - we subscribe to any &quot;volume attached&quot;
or &quot;volume unattached&quot; events, and then record the EBS =&gt; EC2 mappings in a
DynamoDB table. We can then add EBS volume costs to the relevant EC2 instances
before accounting for ECS costs.</p>
<h4>Cross account writes</h4>
<p>Segment uses a separate AWS account for staging, and staging costs are a
significant percentage of the overall AWS bill. We need to write the data
about ECS services in the stage realm to the production Redshift cluster.
This requires allowing the Cloudwatch subscription handlers to assume a role
in production that can write to Firehose (for ECS) or to DynamoDB (for EBS).
These are tricky to set up because you have to add the correct permissions to
the right role in the staging account (<code>sts.AssumeRole</code>) and in the production
account, and any mistake will lead to a confusing permission error.</p>
<p>This also means that you don't have a staging realm for your accounting code,
since the accounting code in stage is writing to the production database.
You can add a second service in stage that subscribes to the same data but
doesn't write it, add extra tests, or decide that you can swallow the occasional
problems with the stage accounting code.</p>
<h2>Rolling up the statistics</h2>
<p>Finally we have all of the pieces we need: tagged resources in the AWS billing
CSV, data about when every ECS event started and stopped, a mapping between ECS
service names and the relevant product areas, and EBS mapping data.</p>
<p>To roll all of this up, I broke out the analysis by AWS product. For each AWS
product, I totaled the Segment product areas, and their costs, for that AWS
product. I recommend breaking out the analysis by AWS product area because each
required a fair amount of massaging - in some cases, an automated tagging rule
that needed to be applied to a subset of resources that were not managed via
Terraform, or in other cases (like the AWS support bill), manually tagging the
entire AWS product with a single product area. EC2 was the most complex because
we also needed to aggregate the EBS and the ECS data.</p>
<p>The data gets rolled up into three different tables:</p>
<ul>
<li>Total costs for a given ECS service in a given month</li>
<li>Total costs for a given product area in a given month</li>
<li>Total costs for a (AWS product, Segment product area) in a given month. For
example, &quot;The warehouses product area used $1000 worth of DynamoDB last month.&quot;</li>
</ul>
<p>For each of these tables, we have a <code>finalized</code> table that contains the
finalized numbers for each month, and a <code>rollup</code> append-only table that writes
new data for a month as it updates every day. A unique identifier in the
<code>rollup</code> table identifies a given run, so you can sum the AWS bill by finding
all of the rows in a given run.</p>
<h3>Errata</h3>
<ul>
<li>
<p>Scripts that aggregate data, or copy it from one place to another, are often
infrequently touched and under monitored. For example, Segment had a script that
copied the Amazon billing CSV from one S3 bucket to another, but it failed on
the 28th of each month because the Lambda handler doing the copying ran out of
memory as the CSV got large. It took a while to notice this because the Redshift
database had a lot of data and the right-ish numbers for each month.</p>
<p>Be sure these scripts are well documented, especially with information about
how they are deployed and what configuration they need. Link to the source
code in other places where they are referenced - for example, any place you
pull data out of an S3 bucket, link to the script that puts the data in the
bucket. Also consider putting a README in the S3 bucket root.</p>
</li>
<li>
<p>Redshift queries can be really slow without optimization. Consult with the
Redshift specialist at your company, and think about the queries you need,
before creating new tables in Redshift. In my case we were missing the right
sortkey on the billing CSV tables. <em>You cannot add sortkeys after you create the
table</em>, so if you don't do it up front you have to create a second table with
the right keys, send writes to that one and then copy all the data over.</p>
<p>Using the right sortkeys took the query portion of the rollup run from about
7 minutes to 10-30 seconds.</p>
</li>
<li>
<p>Initially I planned to run the rollup scripts on a schedule - Cloudwatch would
trigger an AWS Lambda function a few times a day. However the run length was
variable (especially when it involved writing data to Redshift) and exceeded
the maximum Lambda timeout, so I moved it to an ECS service instead.</p>
</li>
<li>
<p>Any time you start writing new data to Redshift, the data in Redshift changes
(say, new columns are added), or you fix integrity errors in the way the data
is analyzed, add a note in the README with the date and information about what
changed. This will be extremely helpful to your data analytics team.</p>
</li>
<li>
<p>The blended costs are not useful for this type of analysis - stick to the
unblended costs which show what AWS actually charged you for a given resource.</p>
</li>
<li>
<p>There are 8 or 9 rows in the billing CSV that don't have an Amazon product
name attached. These represent the total invoice amount, but throw off any
attempt to sum the unblended costs for a given month. Be sure to exclude these
before trying to sum costs.</p>
</li>
<li>
<p>Your company may have paid AWS up front to reserve a certain amount of
capacity. In Segment's case this means several large charges that show up in
the December billing CSV need to be amortized across each month in the year;
the start date for these charges is December 2016 and the end date is December
2017. To find all costs, for, say, March, you need to write a query for all
charges where the start date is before midnight on April 1, and the end date
is on or after midnight on March 1. Then you need to find the percentage of
the unblended cost that was incurred in the time period you care about. Check
with your accounting/analysis team as they may want you to divide a yearly
charge by 12, instead of multiplying by 31/365, to get the percentage that
should apply for a given month.</p>
<p>Subscription costs take the form &quot;$X0000 of DynamoDB,&quot; so they are
impossible to attribute to a single resource or product area. Instead we sum
the per-resource costs by product area and then amortize the subscription
costs according to the percentages. This isn't perfect. If a large
percentage of your bill is reserved up front, this amortization strategy
will be distorted by small changes in the on-demand costs. In that case
you'll want to amortize based on the usage for each resource, which is more
difficult to sum than the costs.</p>
</li>
<li>
<p>AWS does not &quot;finalize&quot; your bill until several days after the end of the
month. You can detect when the bill becomes &quot;final&quot; because the <code>invoice_id</code>
field in the billing CSV will be an integer instead of the word &quot;Estimated&quot;.</p>
</li>
<li>
<p>I chose Javascript for the rollup code initially because it runs on Lambda and
most of the other scripts at the company were in Javascript. If I had realized
I was going to need to switch it to ECS, I would have chosen a language with
better support for 64 bit integer addition, and parallelization and cancellation
of work.</p>
</li>
<li>
<p>It will be difficult to measure the entire bill. Target a percentage of the
costs in the bill, say, 80%, and try to get that measurement working end-to-end.
It's better to deliver business value analyzing 80% of the bill than to shoot
for 100%, get bogged down in the collection step, and never deliver any results.
Again, saying &quot;this is good enough for now&quot; and documenting improvements is
going to be your friend.</p>
</li>
</ul>
<h3>Conclusion</h3>
<p>This was a lot of hard work but at the end Segment has a lot better visibility
into the components of their AWS bill, and the costs of bringing on new
customers. They'll be able to see if a single service or a single product area
is suddenly costing a lot more, before that causes too much of a hit to their
bottom line.</p>
<p>Hopefully this case study will be useful for helping you get a better sense of
your costs!</p>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-79991861-1', 'auto');
  ga('send', 'pageview');
</script>
<footer>
  burke.services 1.16, built using devel &#43;2d20ded584 Sat May 20 00:45:56 2017 &#43;0000
</footer>
</body>
</html>
